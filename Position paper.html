<!DOCTYPE html>
<html>
    <html lang="en"></html>
  <head>
    <meta charset="utf-8" />
    <title>AI ethics for public organisations</title>
    <h2 id="subtitle">A risk-based approach</h2>
  <script src="https://www.w3.org/Tools/respec/respec-w3c" class="remove" defer>
  </script>
  <script class="remove">
   var respecConfig = {
    specStatus: "STMT",
    additionalCopyrightHolders: "Highberg",
    latestVersion: null,
    isPreview: false,
    group: "base",
    logos: [
    {
      src: "https://raw.githubusercontent.com/wjtmollema/Highberg_AI_ethics/main/Highberg_Logo.jpg",
      url: "https://highberg.com/",
      alt: "Highberg",
      width: 240,
      height: 240,
      id: "Highberg-logo",
    }],
    localBiblio: {
    Floridi2019: {
      authors: ["Luciano Floridi"],
      title: "Translating Principles into Practices of Digital Ethics: Five Risks of Being Unethical",
      href: "https://doi.org/10.1007/s13347-019-00354-x",
      status: "",
      publisher: "Philosophy & Technology 32: 185-193",
      date: "2019",
    },
    AI_ACT: {
      authors: ["European Commission"],
      title: "European Union's Artificial Intelligence Act, Regulation (EU) 2024/1689",
      href: "https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32024R1689",
      status: "",
      publisher: "European Commission",
      date: "2024",
    },

    VandePoel2020: {
      authors: ["Ibo van de Poel"],
      title: "Embedding Values in Artificial Intelligence (AI) Systems",
      href: "https://doi.org/10.1007/s11023-020-09537-4",
      status: "",
      publisher: "Minds and Machines 30: 385-409",
      date: "2020",
    },

    Mihalcea_etal2024: {
      authors: ["Mihalcea et al."],
      title: "Why AI Is WEIRD and Should Not Be This Way: Towards AI For Everyone, With Everyone, By Everyone",
      href: "https://doi.org/10.48550/arXiv.2410.16315",
      status: "",
      publisher: "ArXiv Pre-print",
    },

     ThelissenVerma2024: {
      authors: ["Eva Thelisson", "H. Verma"],
      title: "Conformity assessment under the EU AI act general approach",
      href: "https://doi.org/10.1007/s43681-023-00402-5",
      status: "",
      date: "2024",
      publisher: "AI and Ethics 4: 113-121",
    },

    Mantelero2024: {
      authors: ["Alessandro Mantelero"],
      title: "The Fundamental Rights Impact Assessment(FRIA) in the AI Act: Roots, legal obligations and key elements for a model template",
      href: "https://doi.org/10.1016/j.clsr.2024.106020",
      status: "",
      date: "2024",
      publisher: "Computer Law & Security Review: The International Journal of Technology Law and Practice 54: 106020",
    },

    DouglasLaceyHoward2024: {
      authors: ["David M. Douglas", "Justine Lacey", "David Howard"],
      title: "Ethical risk for AI",
      href: "https://doi.org/10.1007/s43681-024-00549-9",
      status: "",
      publisher: "AI and Ethics: 1-15",
      date: "2024",
    },

    WRR2021: {
      authors: ["Wetenschappelijke Raad voor het Regeringsbeleid"],
      title: "Opgave AI. De nieuwe systeemtechnologie",
      href: "https://www.wrr.nl/publicaties/rapporten/2021/11/11/opgave-ai-de-nieuwe-systeemtechnologie",
      status: "",
      publisher: "",
      date: "2021",
    },

    Birhane_etal2022: {
      authors: ["Abeba Birhane et al."],
      title: "Power to the People? Opportunities and Challenges for Participatory AI",
      href: "https://doi.org/10.1145/3551624.3555290",
      publisher:"EAAMO '22: Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization: 1-8",
      date: "2022",
      status: "",
    },

    Panai2023: {
      authors: ["Enrico Panai"],
      title: "The latent space of data ethics",
      href: "https://doi.org/10.1007/s00146-023-01757-3",
      publisher:"AI & Society: 1-15",
      date: "2023",
      status: "",
    },

      Hollanek2024: {
      authors: ["Tomasz Hollanek"],
      title: "The ethico‑politics of design toolkits: responsible AI tools, from big tech guidelines to feminist ideation cards",
      href: "https://doi.org/10.1007/s43681-024-00545-z",
      publisher:"AI and Ethics: 1-10",
      date: "2024",
      status: "",
    },

    OECD2024: {
      authors: ["OECD"],
      title: "Recommendation of the Council on Artificial Intelligence",
      href: "https://legalinstruments.oecd.org/en/instruments/oecd-legal-0449",
      publisher:"",
      date: "2024",
      status: "",
    },

     UNESCO2024: {
      authors: ["UNESCO"],
      title: "Consultation Paper on AI Regulation”: Emerging Approaches Across the World",
      href: "https://unesdoc.unesco.org/ark:/48223/pf0000390979",
      publisher:"",
      date: "2024",
      status: "",
    },

    AP2024: {
      authors: ["Autoriteit Persoonsgegevens"],
      title: "AI & Algorithmic Risks Report Netherlands - Summer 2024",
      href: "https://autoriteitpersoonsgegevens.nl/en/documents/ai-algorithmic-risks-report-netherlands-summer-2024",
      publisher:"",
      date: "2024",
      status: "",
    },
},
    
      github: "https://github.com/wjtmollema/Highberg_AI_ethics", 
        editors: [{ name: "W.J.T. (Thomas) Mollema", url: "https://www.linkedin.com/in/wjtmollema",company: "Highberg", companyURL: "http://www.highberg.com/" }],
        shortName: "Highberg-position paper AI ethics",
        authors: [
    {
      name: "W.J.T. (Thomas) Mollema",
      url: "https://www.linkedin.com/in/wjtmollema",
      company: "Highberg",
      companyURL: "https://www.highberg.com/"}, {
      name: "M. (Marlijn) Mulder",
      url: "https://www.linkedin.com/in/marlijnmulder/",
      company: "Highberg",
      companyURL: "https://www.highberg.com/"}] 
    
   }
   
  </script>

<script src="https://hypothes.is/embed.js" async></script>
    
  </head>
  <style>
    .content-table {
  border-collapse: collapse;
  border-color: #4DC9D9;
  width: auto;
  margin: 25px 0;
  font-size: 0.9em;
  min-width: 400px;
  border-left-width: 40 px;
  border-right-width: 40 px;
  border-radius: 5px 5px 0 0;
  overflow: hidden;
  box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
}

.content-table thead tr {
  background-color: #4DC9D9;
  color: #ffffff;
  text-align: left;
  font-weight: bold;
}

.content-table tbody tr {
  border-bottom: 1px solid #dddddd;
}

.content-table tbody tr:nth-of-type(even) {
  background-color: #f3f3f3;
}
  </style>
  <body>
    <section id="abstract">
      <ul style="list-style-type:square;">
      <li>Highberg’s risk-based approach to AI ethics is relevant for information professionals concerned with AI, compliance, privacy, security, and ethics.</li>
         <li>For public organisations, a risk-based approach for AI ethics is helpful for realising responsible use of AI systems, in line with organizations’ duties under AI Act.  </li>
          <li>The risk-based approach for AI ethics deepens the ethical component of responsible AI and is a next step to the AI Act’s Fundamental Rights Impact Assessments (FRIA).</li>
          <li>Awareness of ethical risks related to the stakeholders surrounding an AI system is necessary for establishing responsible governance for AI systems.</li>
          <li>Combining ethical reflection with risk analysis leads to ethically justified risk-mitigation measures.</li>
</ul>
    </section>
<br />
<br />
<section>
  <h1>Introduction</h1>
<p>With the coming of the AI Act, risk analyses of AI systems are gaining in importance. The AI Act requires public organisations and deployers of high-risk systems to perform an impact assessment that also encompasses ethical considerations. To achieve responsible use of AI systems in the public sector, the proper interplay between law, technology and ethics has to be found.</p>
<p>For ethical use of AI systems, we believe the Fundamental Rights Impact Assessment on its own in the context of the AI Act is not sufficient. Therefore, we devote attention in this position paper to the ethical component of responsible AI and show public organisations how to effectively ask the question ‘Is it desirable?’ with respect to their AI systems.</p>
<p>At Highberg we believe that practical ethics can be combined with risk analyses of AI systems in what we call ‘a risk-based approach to AI ethics’. Our approach makes AI ethics part of day-to-day practices. What distinguishes Highberg’s approach is that it doesn’t give you a series of boxes to check so that you can flaunt with the label ‘ethical’. Highberg’s approach yields a continuous implementation of AI ethics to which an organisation has to be fully committed, otherwise all one does is ethics washing, shirking or dumping [[Floridi2019]]</p>
<p>In recent years, many principles, frameworks and guidelines for AI ethics have been made available. However, we think more is needed to bring AI ethics into practice. This risk-based approach aims to narrow the gap between implementation in public organisations on the one hand and abstract ethical principles on the other. We look closely at the risk-based AI Act by also taking a risk-based route in our ethical approach. This effectively connects the legal and ethical domains. We also translate the risks AI brings with it to ethical reflection, leading to the mitigation of AI systems’ risks in an ethically grounded way. This ethical grounding based on identified risks justifies the answer to the question ‘Is the AI system desirable?’</p>

 </section>

 <section>
 <h1>Highberg's risk-based approach to AI ethics</h1>
 <h2>Four steps between ethical risks and mitigated measures</h2>
 <p>Before diving into the details, we first lay out the basics of our risk-based approach to AI ethics. With the AI Act or the development of an AI system as starting point, analyses  of so-called ethical risks provide insights into the values and perspectives of the organisation and the relevant stakeholders. To make sense of these insights, an  organisation’s ethical values, principles and goals need to be  discovered and highlighted. These ethical pillars guide and frame an organisation’s identification of a risk and the corresponding measures for risk mitigation that are proposed. For example, when using AI in a health care context, values from medical ethics play an important role, such as respect for autonomy and non-maleficence. However, when using AI in another context, for example in the asset management of public space, other values are prominent, like sustainability and inclusivity. But other contexts also bring along varying value tensions.</p>
<p>In short, the risk-based  approach consists of four steps:</p>

<ol>
<li>Broad risk analysis;</li>
<li>Reflection on ethical risks and tensions between values;</li>
<li>Refine or expand risk analysis with input from ethical reflection;</li>
<li>Decision on mitigating measures.</li>
</ol>

<p>But before we unpack the four steps; it is worthwhile to define what an AI system is and where ethics comes into the picture. Throughout the text, we build up the four-step model of the risk-based approach to AI ethics in a schematic image. We illustrate our approach by pointing out the relations between the AI system, the related ethical risks, and the organization’s goals values.</p> 

<h2>Why do AI systems need ethics?</h2>
<p>Starting with the domain of the AI system, under the AI Act, AI systems are defined as “machine-based” and “designed to operate with varying levels of autonomy and [...] may exhibit adaptiveness after deployment” ([[AI_ACT]], Article 3.1).   With the received input, the system produces an output that directly influences its (virtual) environment. For each AI-system, there are harms to be avoided and benefits to be captured. In relation to these harms and benefits, AI ethics is therefore concerned with the societal duties to avoid harm and do good. AI ethics is about the choices made with regard to the ethical responsibilities of stakeholders in the  context of the AI system. </p>
<p>These responsibilities  could be described  as ‘ethical risks’. This is where the domain of the organisation comes into the picture: AI is always embedded into the social role of the organisation deploying it [[VandePoel2020]]  This view is not relevant for all types of technology, but it is especially for AI systems. AI systems function at varying levels of autonomy, and are characterised, for example, by abilities such as inferencing and pattern recognition, which distinguishes them from more 'static' data science or business intelligence applications, such as Power BI dashboards or algorithmically automated calculations. AI systems are also characterised by their dynamic effects of stakeholders interacting with the system. At the same time, AI systems, such as machine learning models, are also influenced by the stakeholders designing and developing the choices made for data collection [[Mihalcea_etal2024]].</p>
<p>Of course there are many different approaches to ethics. The risk-based approach developed here is Highberg’s tool of choice, because it is largely value-agnostic, doesn’t depend on abstract, fuzzy concepts, and is easily tailorable to real-life organisational contexts. And to be clear: this implementation of ethics is not the same as building an AI management system, for example compliant to ISO42001. It’s rather a specific form of practising AI ethics and deepening the ethical domain of responsible AI.</p>

<p>Blindness to these considerations can lead to harmful outcomes and benefits that remain unnoticed. Awareness of this starts with asking the right questions:</p>

<ul style="list-style-type:square;">
<li>do we pursue an AI solution to the problem or opportunity at hand? Will it give a solution of the quality we expect?</li> 
<li> What are the alternatives to developing an AI solution?</li> 
<li> What are the organisational and public values related to the problem the AI system solves?</li> 
<li> Which stakeholders are affected in the distinct phases of the AI system’s development and deployment?</li> 
</ul>

<p>The context of application determines what decisions on responsible use must be made and what aspects are inherent to the usage of specific AI systems. These systems distinguish themselves from other technologies   via the semi-autonomy of their technological processes, and the challenges they give rise to regarding transparency and explainability, and with stakeholder involvement (e.g., in the design, data collection and deployment phases).</p>
<p>Partly because of some of these complications, the AI Act asks public organisations to undertake   a risk-based assessment for responsibly  deploying AI. Before we can introduce the next domain, that of ethical risks, we need to explain how our approach extends the AI Act’s idea of assessing and mitigating AI systems’ risks for the ethical domain.</p>
</section>

<section>
<h1>Moving public AI ethics beyond the AI Act</h1>
<h2>The AI Act's FRIA as starting point</h2>
<p>The AI Act categorises AI systems according to 'risk levels' . You can read more about this in Highberg’s whitepaper on the AI Act (see https://highberg.com/nl/insights/ai-act-voor-overheden). In short:</p>

<ul style="list-style-type:square;">
<li>Some AI systems face no restrictions, while others are banned because of unacceptable risks. Examples of the latter are systems concerning social scoring, facial recognition, biometric identification, and emotion and belief measurements in schools and the workplace (See [[AI_ACT]], Article 5).</li>  
<li>On the other hand, limited risk systems face transparency obligations.</li>
<li>Finally, high risk systems require a conformity assessment before being put on the market and a Fundamental Rights Impact Assessment (FRIA) before being deployed [[ThelissenVerma2024]].</li>
</ul> 

<p>Already in 2021, the Dutch Ministry of the Interior and Kingdom Relations and the Utrecht Data School developed the Fundamental Rights and Algorithm Impact Assessment (FRAIA) for this end, which “creates a dialogue between professionals who are working on the development or deployment of an algorithmic system” (see https://www.government.nl/documents/reports/2022/03/31/impact-assessment-fundamental-rights-and-algorithms). Highberg has several FRAIA-certified consultants which can support public organisations with the process.</p>
<p>To know what risk-level a system is, an impact assessment is needed. We are convinced that a well-done risk analysis before deploying a system, will save you time in the end.  Without an ethical risk assessment, crucial dangers preying on the AI application could be hidden from sight, leading to severe problems after deployment. In our current context, it is relevant to emphasise that governmental organisations, private organisations that fulfil public duties, and deployers of high-risk systems are obliged to conduct a FRIA prior to bringing an AI system into production [[Mantelero2024]] [[AI_ACT]] (Article 27). The FRIA provides a good start but has its limitations.</p>

<h2>Limitations of FRIA's</h2> 
<p>The FRIA targets the impact of an AI system on fundamental human rights. It prescribes the implementation of measures to prevent risks that can already be identified in the design phase and are to be mitigated. Under the AI Act, ‘risk’ is the combination of the gravity of a harm and how likely it is to occur [[AI_ACT]] (Article 3.2). The benefit of the FRIA is that it enables ethical risks to be tackled before the AI system is taken into production [[Mantelero2024]].  Currently, a new development is the ‘DPIAMA’ (https://highberg.com/capabilities/risk-resilience-compliance/digital-law-privacy-and-ethics/dpiama), which combines a Data Protection Impact Assessment (DPIA) with a FRIA for the same algorithmic system, leading to a double efficiency gain: firstly, because of the overlap between DPIA and FRIA, and secondly, because of bundling both assessments and resulting measures into a single activity.</p>
<p>However, for ethical AI throughout the system’s lifecycle, a successful one-time FRIA is not enough. It only targets risks, possibly eliminated in the development or design phase of high-risk AI systems [[AI_ACT]] (Article 9.3).  The risk thus becomes actual once the AI system is designed in a certain way. But a risk becoming actual is not the same as the harm that results when the risk occurs. Risk management in this sense is about addressing the risks before they cause harm, rather than specifying how to deal with actual harm done to stakeholders and how to remedy that. It is also worth noting that, for high-risk AI systems, the AI Act introduces an obligation to report serious incidents [[AI_ACT]] (Article 73). </p>
<p>Therefore, the FRIA cannot mitigate any novel ethical risks that emerge, or risks that turn into harm, once the AI system is up and running and in interaction with stakeholders.</p>
<p>This means that for successful responsible AI and compliance to the AI Act, risk identification and management are but the very beginning. In the following sections, we hence specify how to practice and implement AI ethics after initial compliance to the AI Act has been reached.  This results in the successful implementation and responsible use of AI systems.</p>
</section>

<section>
<h1>Ethical risks and reflection on values and goals</h1>
<p>The concept of ‘ethical risk’ is the first step towards making AI ethics tangible and practicable. This term is therefore in need of a definition. An ethical risk is a risk concerning an AI system that can lead stakeholders related to the system to fail their ethical responsibilities towards other stakeholders [[DouglasLaceyHoward2024]]. In our schema, this introduces the ‘domain of ethical risks’. In other words, ethical risks are all the ethically negative ways in which an organization’s AI system can affect public values and stakeholders. The deployment of the AI system produces a tension between public values or has a detrimental effect on one of them, or on the welfare or wellbeing of stakeholders. The first step of the risk-based approach is to conduct a broad risk analysis to inquire into how the AI system affects stakeholders and public values.</p>
<p>In Figure 1 we highlight several types of ethical risks regarding AI systems that especially concern public actors and a failure of responsibilities to their stakeholders.</p>
<p>Deciding whether a situation is a risk for an organisation or in a situation specific for an organisation depends on the ethical framework the organisation is using. Your ethical framework is like a pair of glasses that helps discern risks and mitigation strategies, but which may downplay other possible risks  . Knowing what frame is used within your organisation thus helps deciding what risks are relevant, but also what measures to take. But: knowing the framework also yields information about situations that are not considered risks for the organisation itself, but in which other stakeholders can be at risk. When the used framework is aligned with organisational goals and ethical responsibilities, you become able to formulate measures that resonate with your organisation, your employees and your stakeholders.</p>

<figure id="figure1">
  <img src="https://raw.githubusercontent.com/wjtmollema/Highberg_AI_ethics/main/ethical_risks.jpg" width="600" height="1000" alt="Risk-based approach to AI ethics">
  <figcaption>Some prominent ethical risks involved with AI systems.</figcaption>
</figure>

<p>Identifying ethical risks for when the AI system is up and running starts a process of ethical reflection that should be embedded in your organisation. Reflecting on how the organisation’s (public) values and goals frame the ethical risks is the second step of the risk-based approach (see Figure 2). One should ask: ‘Why are these factors and consequences risks for us?’ ‘Why are these risks inacceptable to us? What organisational values and goals underpin that judgment?’ Personal, organisational and public values supply a frame in which something is perceived as a risk, because of how the harm that follows the risks affects those values. The values and goals that are part of the organisation frame the domain of ethical risks. Because they are embedded in the organisation, they provide what counts as a negative effect of AI systems on (public) values and stakeholders.</p>
<p>Following this identification of organizational values and goals, the third step of the risk-based approach refines the initial identification of ethical risks (see Figure 2). Depending on the values you bring along, some risks may be more, or less, ethically relevant. Following this refinement of the risks’ ethical valuation, ways of risk mitigation can be specified that go beyond elimination of the risk alone: they will be deeply tied to your organisation’s core values and the goals that are tried to be achieved by using an AI system. As AI is  system-technology, the ethical risks involved with AI systems have technical, social and legal components [[WRR2021]]. The crux of the matter is to bring these technical, social and legal components together based on the public values your organisation aims to foster in the formulated responses. The set of formulated responses based on the ethically refined ethical risk scop is the input for the final step.</p>

<figure id="figure2">
  <img src="https://raw.githubusercontent.com/wjtmollema/Highberg_AI_ethics/main/full_approach.jpg" width="1200" height="700" alt="Risk-based approach to AI ethics">
  <figcaption>Moving from the identification of ethical risks to ethical risk mitigation.</figcaption>
</figure>

<h2>From AI's ethical risks to decisions on mitigating measures</h2>
</p>The refined scope of ethical risks and the initial formulation of responses brings us to the final step of the risk-based approach to AI ethics (see Figure 2). The final step is a value-based ethical trade-off. Every ethical reflection that must end as a decision involves making a trade-off regarding the related value tension(s). In the case of the domain of ethical risks that AI ethics deals with, not all risks can be eliminated, and sometimes, an enormous benefit is worth the risk of a (minimal) harm occurring. This step comes last because this trade-off can only be made based on your organisation’s ethical principles, (public) values and goals. Without ties to these goals and the values at your organisation’s core, accounting for the benefits and burdens of your AI system is impossible. The final step outputs a decision that proposes an ethically weighed off risk measure targeted at the AI system’s ethical risk that is built upon:</p>

<ul style="list-style-type:square;">
<li>the first step’s identification of how the AI system affects stakeholders and public values;</li>
<li>the second step’s organisation’s (public) goals and values that underpin the ethical reflection; and</li>
<li>the third step’s risk scope that was ethically refined based on the identified goals and values and the set of possible responses to the ethical risks that follow from it.</li>
</ul>

<p>To summarise, the risk-based approach to AI ethics that we have developed and motivated here consists of the following steps.</p>
<ol>
<li>Broad risk analysis: What is the potential negative impact of the AI system?;</li>
<li>Reflection on ethical risks and tensions between values: First, reflect on the organisational and public values you that are of importance to your tasks and services. Then use this reflection to identify how the ethical risks concerned with the AI system affect your organisation’s distinct value profile;</li>
<li>Refine risk analysis with input from ethical reflection: Specify the risks’ technical, legal, social and ethical dimensions and formulate possible responses relate to your organisational values;</li>
<li>Decision on mitigating measures: Weigh off possible measures and prepare to address and be accountable for any remaining harms and risks.</li>
</ol>
<p>Although we advise that AI ethics is practiced well before the system is deployed, this reflection plan is general enough to be accommodated to any phase of the AI system lifecycle. Whether these questions are asked during design or evaluation of the system, co-designing answers with your intraorganizational stakeholders will prove beneficial.</p>
<p>Finally, note well that the AI Act itself pays little attention to stakeholder participation in AI development and deployment, which is why stakeholder management and participation should be considered as central to AI ethics after the FRIA is first completed [[Mantelero2024]]. Note that in the process of including stakeholders, stakeholders can also be represented by an emissary they support. Especially because in the case of public organisations operating in the social domain, stakeholder involvement is the central ethical concern.</p>
<p>Striving for ‘participatory AI’ can therefore be a helpful result of ethical reflection: affected stakeholders need to be involved in the design of AI system and the evaluation of the system’s  deployment or should at least be adequately consulted [[Birhane_etal2022]].  For example, stakeholder involvement can contribute to minimizing algorithmic bias in the data gathering phase by introducing stakeholders in the data curation process. Stakeholder involvement can also have positive effects on the design phase by introducing them in the process of determining the right classificatory outputs for AI systems. Likewise, if stakeholders are subjected to the output of an AI system, they should be involved in the evaluation  of the system too: together with stakeholders, public organisations can evaluate whether any unforeseen problems emerged or how the continuously learning AI system can be improved.</p>

<h2>A note on succesful implementations of AI ethics</h2>
<p>How can you put the risk-based approach  to good use in your organisation?</p>
<p>The past few years have  seen a proliferation of different functions, roles and tools for AI ethics. Ranging from (AI) ethics officers, ethical commissions, algorithmic risk boards, data-ethics expertise groups [[Panai2023]], frameworks, principles (see, e.g. [[OECD2024]] [[UNESCO2024]] [[AP2024]]) and toolkits [[Hollanek2024]],  these all have their pros and cons. However, our vision is that whatever the implementational format, making room in the business model of AI development and deployment for the necessary ethical conversations about the responsible use of AI, is the main factor of success. But, making room for AI ethics should be tuned to the organisation’s and its AI systems’ context. In short, tailoring the implementation of AI ethics is advised.</p>
<p>This is where Highberg’s consultants can make the difference.  Within Highberg we strive to address and take on all aspects of responsible digital and organisational transformations. We bring a diverse pallet of perspectives and expertise to help public and private actors pragmatically harness AI’s potential and mitigate its risks. This combination of expertise and diversity is what enables making real impact. Highberg’s approach yields more than a tactic for checking the ethical box: it results in a continuous implementation of AI ethics to which an organisation must be fully committed. The continuity of this implementation is enabled by embedding it in the existing control or ‘plan-do-check-act’ cycles.</p>
<p>This shows that ethics is more than a good conversation but does start with it. For realising successful implementation of AI ethics, the underlying organisation and public values have to be excavated and transformed into practical tools for ethical risk analysis. We want to make sure that your AI system is ethically validated throughout its entire lifecycle.</p>   

<p>It is time to give public actors’ AI use the ethical care it deserves.</p>
 </section>
<section>
<h1>Acknowledgements and author information</h1> 
<h2>Acknowledgements</h2> 
<p>Our thanks go out to our colleagues Sabine Steenwinkel-Den Daas, Sebastiaan Streng, and Erik van Zegveld for their helpful feedback and directions during the writing process. Thanks also to Luna van Vilsteren for her contribution to some of the visualisations figuring in the text.</p>
<h2>Author information</h2> 
<p>TO BE DETERMINED</p>
      </section>
 </section>
  </body>
</html>
